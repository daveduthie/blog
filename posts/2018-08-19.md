## Synchronization via Timely Dataflow

Timely dataflow is meant to be a fairly handy way to write dataflow computations, but did you know that it can be used to implement other communication and coordination patterns too?

It can! Not that you *should* do this, but it beats having to shell out to ZooKeeper to do something simple.

There are at least two examples I can think of: barrier synchronization and event serialization (putting things in an order, rather than turning into bytes). We are just going to do barriers today, because it is sunny out.

### Barrier Synchronization

Let's take a simple example that comes up a lot in the evaluation of timely dataflow programs: you want to measure how long a computation takes, but to do this you need to get all workers to start their timers at the same time.

```rust
fn main() {

    timely::execute_from_args(std::env::args(), move |worker| {

        let timer = std::time::Instant::now();

        // build some computation, ..
        worker.dataflow(|scope| {

        })

        // Each worker loads some data, takes non-uniform time.
        std::thread::sleep(Duration::from_secs(worker.index() as u64));

        println!("{:?}\tworker {} loaded", timer.elapsed(), worker.index());

        // run to completion
        while worker.step() { }

        println!("{:?}\tworker {} complete", timer.elapsed(), worker.index());

    }).unwrap(); // asserts error-free execution;
}
```

What is the gap between the two printed lines? We would like to think that is the time it takes to do the computation, but it is a bit more subtle than that. It is actually the time from when a worker finishes loading *its* data, until the whole computation completes. That includes time spent waiting for other workers to load up their data, which can be a very different amount of time.

The above program (with an empty dataflow) does not actually cause a problem (timely dataflow is too clever, and realizes that without a dataflow each worker can finish independently). But if we make it a bit more non-trivial, just a touch of data, we see (with two threads):

    Echidnatron% cargo run --example barrier-ex -- -w2
       Compiling timely v0.6.0 (file:///Users/mcsherry/Projects/timely-master)
        Finished dev [unoptimized + debuginfo] target(s) in 4.28s
         Running `target/debug/examples/barrier-ex -w2`
    6.068µs Worker 0 loaded
    1.002544178s    Worker 1 loaded
    1.003165254s    Worker 1 complete
    1.003172394s    Worker 0 complete
    Echidnatron%

We can reason a bit and say that we should look for the smallest gap between loading and completing (here: Worker 1), but this feels like a bit of a cheat.

You might have noticed that we actually got some pretty sweet synchronization out of

```rust
        // run to completion
        while worker.step() { }
```

where we run the dataflow graph to completion. We've got the ability to run multiple dataflow graphs; why don't we just make a new dataflow that does nothing and run it to completion to synchronize the workers?

```rust
        // synchronize all workers with a barrier.
        let probe = worker.dataflow::<(),_,_>(|scope| {
            let (_, input) = scope.new_input::<()>();
            input.probe()
        });
        while !probe.done() { worker.step(); }
```

This fragment has each worker build a new dataflow graph, with really not much in it. The timestamp type is `()`, and there is one input with a datatype of `()`. We never introduce any data, and just `probe()` the output to see when we can be sure that no other worker will produce any data. This happens only once each of the other workers have created and dropped their input handle (the `_` above).

This actually works. If you drop this hunk of code in to a timely dataflow worker, the workers will chill out here until all other workers have also reached this point. Once they've all reached the point, all workers release and are somewhat synchronized (at least, they are all done with whatever they were doing beforehand).

The above fragment is a bit wordy, so we could actually go and package it up into a timely dataflow method for you.

I did one better and put together a re-usable barrier (coming soon to a PR near you), in case you want to syncronize the workers multiple times (and don't want to create new dataflows each time). It's almost the same as the example above, except that the timestamp type is `usize` so that you can tick upwards each time you re-use the barrier.

```rust
//! Barrier synchronization.

use ::Allocate;
use progress::timestamp::RootTimestamp;
use progress::nested::product::Product;
use dataflow::{InputHandle, ProbeHandle};
use dataflow::scopes::root::Root;

/// A re-usable barrier synchronization mechanism.
pub struct Barrier<A: Allocate> {
    round: usize,
    input: InputHandle<usize, ()>,
    probe: ProbeHandle<Product<RootTimestamp, usize>>,
    worker: Root<A>,
}

impl<A: Allocate> Barrier<A> {

    /// Allocates a new barrier.
    pub fn new(worker: &mut Root<A>) -> Self {
        use dataflow::operators::{Input, Probe};
        let (input, probe) = worker.dataflow(|scope| {
            let (handle, stream) = scope.new_input::<()>();
            (handle, stream.probe())
        });
        Barrier { round: 0, input, probe, worker: worker.clone() }
    }

    /// Blocks until all other workers have reached this barrier.
    ///
    /// This method does *not* block dataflow execution, which continues to execute while
    /// we await the arrival of the other workers.
    pub fn wait(&mut self) {
        self.round += 1;
        self.input.advance_to(self.round);
        while self.probe.less_than(self.input.time()) {
            self.worker.step();
        }
    }
}
```

As indicated in the comment, the `wait()` method has the potentially cool property that it does not block dataflow execution. We keep executing the dataflow (`self.worker.step()`) waiting for the synchronization signal, which means that we are also actively running all other live dataflows. Compare this with a more traditional distributed barrier, which might cause each worker to lock-up until all other workers are ready, doing nothing at all while they wait.

Let's look at a program which uses this new `Barrier` type:

```rust
extern crate timely;

use std::time::{Instant, Duration};

// Note: in a branch; can't have yet!
use timely::synchronization::Barrier;

fn main() {

    timely::execute_from_args(std::env::args(), move |worker| {

        let timer = Instant::now();
        let mut barrier = Barrier::new(worker);

        loop {
            // screw up synchronization, then re-synchronize.
            std::thread::sleep(Duration::from_secs(worker.index() as u64));
            let elapsed1 = timer.elapsed();
            barrier.wait();
            let elapsed2 = timer.elapsed();
            println!("Worker {}: {:?}\t{:?}", worker.index(), elapsed1, elapsed2);
        }

    }).unwrap(); // asserts error-free execution;
}
```

This program starts multiple workers, and repeatedly screws up the synchronization between them and then resynchronizes. It prints out the de-synchronized times and the synchronized times. Ideally the former should be all different, and the latter should be pretty much the same.

    Echidnatron% cargo run --example barrier-ex -- -w2
       Compiling timely v0.6.0 (file:///Users/mcsherry/Projects/timely-master)
        Finished dev [unoptimized + debuginfo] target(s) in 4.14s
         Running `target/debug/examples/barrier-ex -w2`
    Worker 0: 624.773µs 1.003743907s
    Worker 1: 1.003576543s  1.003751088s
    Worker 1: 2.005013854s  2.005141737s
    Worker 0: 1.003813969s  2.005136371s
    Worker 0: 2.005176509s  3.010288135s
    Worker 1: 3.010185988s  3.010295034s
    Worker 1: 4.015362201s  4.015469842s
    Worker 0: 3.010313215s  4.015464516s
    ^C
    Echidnatron%

Yeah, pretty good.

### Event Serialization

Coming soon!