## Timely Dataflow Architecture

This post will be a series of posts on details about timely dataflow's internal architecture, or at least salient aspects of the current design, as well as bits of roadmap for where things are going next. The intent is to fill it out with more detail as parts are completed, while still managing to sketch what to expect in the near term.

Here are the rough subject headings. Those with links are "done" and those without links are still in progress.

0. [**Background**](https://github.com/frankmcsherry/blog/blob/master/posts/2018-09-26.md#background-19112018): Timely dataflow is, perhaps obviously, a dataflow system. What this *means* is perhaps less obvious, as the term has been somewhat overused recently. We will walk through core aspects of the design of timely dataflow programs, and why we hope these decisions may lead to better (lower latency, higher throughput) computations.

1. [**Dataplane**](https://github.com/frankmcsherry/blog/blob/master/posts/2018-09-26.md#dataplane-19112018): Timely dataflow's *dataplane* is the part of the system dedicated to moving data between workers. While this is likely an important part of any data-intensive system, it is surprisingly important in timely because of timely's data-driven execution. Where other systems can afford to marshall data and round-trip it through multiple threads (and even processes!) if we want to hit microsecond latencies we will need to do things more carefully.

2. **Scheduling**: In a dataflow design the execution model can be purely data-driven: one only needs to do work in response to the arrival of data, and the work one needs to do is fairly clearly defined. This is less important in simple benchmark programs (PageRank! WordCount! YSB!) but absolutely critical in more complex programs that involve thousands of dataflow stages, or in settings where large numbers of concurrent dataflow execute.

3. **Progress**: Timely's main coordination mechanism is "progress tracking", which circulates background information about records and capabilities produced and consumed by operators, in such a way that all operators asynchronously maintain conservative views of the possible messages they might receive in the future. Without some care, this has the potential to swamp the computation itself (as demonstrated in [the Naiad paper](https://dl.acm.org/citation.cfm?id=2522738)).

### Background (19/11/2018)

Timely dataflow is a framework for doing parallel computation. It's a bit more than that, but at its heart it provides infrastructure for multiple largely independent workers to accomplish computation, while minimizing the friction among the workers who would very much like to act as if they were actually independent.

Timely programs are structured as [SPMD](https://en.wikipedia.org/wiki/SPMD) programs, an abbreviation for "single program, multiple data". A program is written as if it were a single-threaded computation, and then multiple copies of this program are started with likely different input data. As each program runs, there are various collective moments where data move from one stage of the computation to another, and where the system can intervene and re-shuffle the data between workers.

A system that randomly re-shuffles your data sounds like a disaster to understand, and so timely dataflow programs try to make this easy to understand by using dataflow idioms. In dataflow computations the data are the only information that move between the parts of your computation. This rules out idioms like global mutable state, wonky control flow, and other possibly anti patterns. At the same time, by ruling these out the results are programs that are more amenable to distributed execution, because it is the data themselves that drive the computation.

#### An Example

Let's look at a relatively simple example (relative to timely dataflow programs, not to hello-world programs):

```rust
extern crate timely;

use timely::dataflow::{InputHandle, ProbeHandle};
use timely::dataflow::operators::{Input, Exchange, Inspect, Probe};

fn main() {
    // initialize and run a timely dataflow.
    timely::execute_from_args(std::env::args(), |worker| {

        // Handles bridge dataflow and imperative drivers.
        let mut input = InputHandle::new();
        let mut probe = ProbeHandle::new();

        // create a new input, exchange data, and inspect its output
        worker.dataflow(|scope|
            scope.input_from(&mut input)
                 .exchange(|x| *x)
                 .inspect(move |x| println!("hello {}", x))
                 .probe_with(&mut probe);
        );

        // Worker-specific information.
        let index = worker.index();
        let peers = worker.peers();

        // introduce data and watch!
        for round in 0..10 {
            if round % peers == index {
                input.send(round);
            }
            input.advance_to(round + 1);
            while probe.less_than(input.time()) {
                worker.step();
            }
        }
    }).unwrap();
}
```

In this example, each worker performs the closure that begins `|worker| {`. In this program there are roughly two parts to this close: we first describe a dataflow, and we then feed that dataflow with data. We create a dataflow that reads some input, exchanges the data, prints the results, and then "probes" the result. This is going to be our computation, modest as it is. The rest of our closure feeds in some input data, varying based on `index`, and calls `worker.step()`.

We are trying to build one program that works for each worker, but for them to have different behaviors we will need to let them distinguish between themselves. Workers have access to a method `index()` that indicates their specific rank in the range `0 .. worker.peers()`, where the `peers()` method tells them how many workers are involved in the computation. In principle the program could look really hard at `index()` and do something totally different as a result, which will not work out well for anyone involved. We won't do this.

Our closure not only describes a dataflow computation, but it also programs the scheduler as well. Perhaps the most important line above, and the most understated, is

```rust
    worker.step();
```

The `worker.step()` call is where timely dataflow takes over and schedules each dataflow operator for a little bit of time. This allows our closure to interactively alternate between supplying more input and stepping the computation. At any point the closure can also just walk away, by returning, and the timely worker will close its inputs and then spin on `worker.step()` until the computation is complete.

#### Some context: Numbers!

The main reason we are doing what we are doing is in the name of performance, so let's set some expectations. Couldn't we just use something like [Heron](https://github.com/apache/incubator-heron), a modern stream processor that exchanges records using [up to 80 milliseconds, across six threads, and four processes](http://strymon.systems.ethz.ch/assets/pdf/fischerf-heron.pdf)?

Our example above is a fairly simple exchange and coordination microbenchmark. Let's see how long it takes to run (once compiled):

    Echidnatron% time cargo run --release --example hello
        Finished release [optimized + debuginfo] target(s) in 0.06s
         Running `target/release/examples/hello`
    hello 0
    hello 1
    hello 2
    hello 3
    hello 4
    hello 5
    hello 6
    hello 7
    hello 8
    hello 9
    cargo run --release --example hello  0.06s user 0.04s system 78% cpu 0.131 total
    Echidnatron%

The 131ms running time is pretty big. It turns out this is mostly loading up the binary. Let's make the program a bit more trim, by removing the output printing and by letting the number of rounds vary upward as a user-supplied parameter.

    Echidnatron% time cargo run --release --example hello -- 1000
        Finished release [optimized + debuginfo] target(s) in 0.06s
         Running `target/release/examples/hello 1000`
    cargo run --release --example hello -- 1000  0.06s user 0.04s system 78% cpu 0.133 total
    Echidnatron%

One thousand rounds takes as much time as ten rounds, which should be a clue that we haven't quite gotten past the start-up overhead yet.

    Echidnatron% time cargo run --release --example hello -- 1000000
        Finished release [optimized + debuginfo] target(s) in 0.06s
         Running `target/release/examples/hello 1000000`
    cargo run --release --example hello -- 1000000  1.58s user 0.05s system 97% cpu 1.664 total
    Echidnatron%

One million rounds takes 1.664 seconds, meaning roughly 1.664 microseconds per iteration (none of the rounds are performed concurrently).

This number goes up as we add more workers, two workers taking

    Echidnatron% time cargo run --release --example hello -- 1000000 -w2
        Finished release [optimized + debuginfo] target(s) in 0.06s
         Running `target/release/examples/hello 1000000 -w2`
    cargo run --release --example hello -- 1000000 -w2  5.35s user 0.06s system 192% cpu 2.807 total
    Echidnatron%

and the number will go up even further if we involve multiple processes and force the data through the kernel TCP stack.

The point is not meant to be that timely dataflow is currently incredibly fast (it is pretty good), but that the computation is lean enough that we *should* worry about adding random overheads. Paying attention to these details will pay dividends in latency and throughput, because everything beyond a microsecond per iteration is just overhead, in this case at least.

### Dataplane (19/11/2018)

The subject of this postlet is timely dataflow's "data plane", or how timely dataflow moves data around. More than this, though, it is about timely dataflow's aim to be a "dataplane system", one in which the movement of data is what drives the behavior of the system. This contrasts with more traditional systems with a "control plane" that looks more like a scheduler that tells its workers "now do this"; "now do this"; "now do this".

Systems designed as dataplane systems trim away substantial fat from the critical paths through their computations, both reducing latency and improving throughput. The most common trade-off is in run-time flexibility: having commited to specific computations, and specific work in response to received data, it is more complicated to course-correct mid-computation. In the context of a streaming data processor, we are going to take this as an acceptable trade-off (but you should always understand the trade-offs you make!).

This post will target and explain dataplane idioms, things like "zero allocation", "zero copy", and "run to completion", and how they fit in to the timely dataflow design. The goal in this post at least is to lay out some terminological groundwork, and talk through several of the changes that have been made in timely dataflow to make it a leaner system that takes full advantage of its dataflow underpinnings.

#### Zero-allocation

We would very much like to avoid allocation whenever possible. Allocation is the sort of thing that can bite you in surprising ways, especially when you unintentionally yield control to the underlying memory allocator for maintenance tasks. Of course, user code may ask to do allocations, nothing to be done about that, but we would love to avoid any additional allocations that the user doesn't ask for.

As our workers receive communications from other workers, and prepare outgoing communications to other workers, we aim to avoid all allocations except in the case that we have filled what buffers we currently have and need to acquire more in order to avoid blocking. There is a reasonable discussion to have about whether perhaps you *should* block when you start to run out of memory, and this is substantially more complicated than the "yes, you should block" people would have you believe.

#### Zero-copy

As data make their way from the network device up to user code, we have many opportunities to copy the data around. It is really easy to do. The previous version of timely dataflow's communication infrastructure had [about seven copies](https://github.com/frankmcsherry/timely-dataflow/issues/111) as data went from "received at network device" through "posted back to network device".
But, these copies cost, and in a perfect world we would leave the data just where it landed when it arrived, and point the user code at it rather than make copies.

This is in a bit of conflict with Rust's approach to ownership, in that "where it landed" is likely not memory to which we can just hand out joint ownership, but it does perhaps line up with Rust's borrowing philosophy which says "you can use this memory briefly". This is something we will end up working around rather than align perfectly with.

#### Run-to-completion

Ideally, a worker repeatedly invokes bits of dataflow or user code that it can "run to completion". This means that the code doesn't block, but also that by the time the code has completed the input resources it needed (i.e. the data on which it acts) can now be released. This is especially valuable if these resources are scarce, for example buffers associated with the network device.

This is in contrast with designs that might take the associated data and pass ownership to another thread of control, which gives up the urgency of completing the work, and requires further coordination and some optimism about how promptly the work will complete.

Our ideal `worker.step()` method might look something like this (totally made up):

```rust
fn step(&mut self) {
    // 1. collect inbound data.
    let dataz = self.communication.poll();

    // 2. apply relevant operators.
    for message in dataz.messages() {
        self.operator[message.op].respond_to(
            &mut message,
            &mut self.communication
        );
        message.release();
    }

    // 3. flush outbound data.
    self.communication.flush();
}
```

In this framing, we snag inbound data from whatever communication infrastructure we are using (TCP, RDMA, shared memory), show each (binary) message to the operator that should respond to it (as well as a handle to the communication infrastructure, so it can transmit), and then flush any buffered outgoing communication.

In this design, we have very clear constraints about what each operator can see and for how long. They are presented with a reference to received data (`message`) and Rust's lifetime guarantees re-assure everyone that by the time `respond_to()` returns no one will have a lingering reference to anything in `message`, and by the time the for loop completes no one will have lingering references into `dataz`.

This design is probably pretty great, whenever it is easy to write a `respond_to()` method that acts on messages containing bytes. In timely, this isn't especially easy, for a few reasons:

1. These operators are written by users that want to act on Rust types, not just bytes. They could probably act on bytes, but what a pain in the butt and probably the best thing to do is pick some binary format and deserialize everything and hey why not JSON as the format since we are anyway planning on TAKING FOREVER.

2. Many of these operators have non-trivial invariants they need to temporarily break as they process messages, and it can be expensive to re-establish the invariants for each message. Worse, the associated control flow is often tedious to write in a data-driven style (all owned state must be shared under `Rc<RefCell<_>>` wrappers).

Otherwise, it is a good plan and we will try and stick with its spirit.

#### Proposal: Batched Dataplane

The dataplane design above conflates two potentially distinct concepts: i. "what tasks should be scheduled" and ii. "in what order should tasks be scheduled". Each time through `worker.step()` it is important that we schedule all tasks that will receive data, so that we can reclaim the resources backing the data, but we arguably have some flexibility about how we schedule them. We do not *need* to schedule them in the order they were received, but we should schedule all of them.

This leads to what I'm calling "batched dataplane", where in effect we write a new step 2 above:

```rust
    // 2. apply relevant operators.
    for message in dataz.messages() {
        self.operator[message.op].enqueue(message);
        self.activate(message.op);
    }
    for operator self.activated_operators() {
        operator.schedule();
    }
```

This allows us to execute dataflow operators written in a more natural style (as imperative code, rather than reactive code), schedule them as some more general scheduler sees fit, but still gives us some of the most significant benefits of a dataplane implementation, namely more predictable resource usage and timing.

#### Proposal: `Bytes` knock-off

There is [this `bytes` crate](https://crates.io/crates/bytes) that does a crap-ton of things, but abstractly it allows one to start from a `Vec<u8>` and peel of multiple disjoint owned "regions" of the underlying allocation. These regions can each be mutated independently, where the disjointness ensures that there will be no data races.

We need something similar in that `dataz` above is more likely to be a contiguous allocation than it is to be `message`-sized independent allocations. For various reasons, it made more sense to write [a quick `bytes` knock-off](https://github.com/frankmcsherry/timely-dataflow/blob/master/bytes/src/lib.rs) than to use the existing crate (based on the provided features at the time of evaluation).

The main functionality of the crate is a type:

```rust
/// A thread-safe byte buffer backed by a shared allocation.
pub struct Bytes {
    /// Pointer to the start of this slice (not the allocation).
    ptr: *mut u8,
    /// Length of this slice.
    len: usize,
    /// Shared access to underlying resources.
    ///
    /// Importantly, the boxed asset is unavailable except through
    /// the methods of the struct, which each test for unique ownership.
    sequestered: Arc<Box<Any>>,
}
```

with methods

```rust
impl Bytes {
    /// Create a new instance from a byte allocation.
    pub fn from<B>(bytes: B) -> Bytes where B: DerefMut<Target=[u8]>+'static { .. }
    /// Extracts [0, index) into a new `Bytes` which is returned, updating `self`.
    pub fn extract_to(&mut self, index: usize) -> Bytes { .. }
    /// Recover the underlying storage.
    ///
    /// This method either returns the underlying storage if it is uniquely held,
    /// or the input `Bytes` if it is not uniquely held.
    pub fn try_recover<B>(self) -> Result<B, Bytes> where B: DerefMut<Target=[u8]>+'static { }
}
```

These methods plus `Deref` and `DerefMut` implementations (plus a few others that help avoid allocations) give us the ability to take large-ish contiguous allocations and slice them up according to message boundaries and hand out "owned" data backed by the shared allocation. Once each of the operators have run and their `Bytes` instances have been dropped, we should be able to `try_recover()` the allocation and re-acquire the backing memory

Arguably, this is mostly in support of helping us dance around Rust's ownership discipline. It would have been *much* nicer to directly invoke callbacks on their associated slice of the incoming data, but I haven't seen a good ergonomic solution for that (specifically, how to trick users into programming using that interface). That being said, I should probably make sure that one *can* write in that style if they are able (`todo.push(self)`).

#### Proposal: Abomonation

If you are familiar with [abomonation](https://github.com/frankmcsherry/abomonation), then i. great, and ii. not-so-great. It is a fun serialization mechanism that (believes it) uses the the same in-memory representation as Rust, and largely works by correcting internal pointers. The relevant method here is

```rust
pub unsafe fn decode<T: Abomonation>(bytes: &mut [u8]) -> Option<(&T, &mut [u8])> { .. }
```

which starts from a `&mut [u8]` and produces a `&T` (in the error-free case). The input bytes need to be mutable so that we can update pointers, and the resulting reference should not be mutable because the only guarantee is that the result "looks like" an instance of `T`, not that it actually is one; triggering a re-allocation of an internal vector would cause unknown (and undefined) horror. The `Abomonation` trait is only implemented for a subset of types for which we believe access to `&T` is safe (e.g. yes `Vec<_>, but not `Rc<_>`).

Despite all of those caveats (and the explicit `unsafe`) this is one of the only zero-allocation, zero-copy deserialization mechanisms I'm aware of that returns references to actual Rust types, rather than proxy wrappers. Timely also supports [bincode](https://crates.io/crates/bincode) for deserialization, but it will use allocations and copies. One could look into what it would take to line up with e.g. CapnProto or FlatBuffers, but to the best of my understanding the corresponding user code would need to be written against their wrapper types rather than Rust types, which was determined to be an ergonomic non-starter.

#### In sum

Bringing these pieces together, we have an approach that starts from raw binary allocations, and with no copies, no allocations, runs user operator code to completion on input `&T` data. This may come across as a bit of a tangle at the moment (surely), and the talking through is meant at least to tease out any of the weak points in the argument (e.g. why "batched dataplane" vs "just dataplane").

The [zero-copy intra-process allocator](https://github.com/frankmcsherry/timely-dataflow/blob/master/communication/src/allocator/zero_copy/allocator_process.rs) lets us try this out, without actually sorting out several other dataplane issues (we want to sneak away from kernel TCP to avoid actual copies). In this allocator, processes communicate by serializing their data to shared binary ledgers (no, not that kind) from which recipient workers deserialize.

A lightly modified exchange micro-benchmark (like the above, but with more attention paid to the post-exchange operator to avoid copying the resulting buffers) stabilizes at just a few thousand total allocations, even run indefinitely. There were a few gotchas here (avoid Rust's MPSC queues, avoid un/re-boxing the backing memory via `Bytes` helper methods), but it is at a nice spot where we can track down allocations and determine if they are truly necessary, and optimize them where appropriate.

### Scheduling (TODO)

### Progress (TODO)