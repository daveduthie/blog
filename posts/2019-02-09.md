## Abstract Algebra in Differential Dataflow

This one is a bit of a risk, but I've gotten the impression that some of you are actually into mathematics. I suppose, reasonably, anyone who is not into mathematics has probably long since left this blog, and is now living a happy and cheerful life enjoying other things.

But, for those of you who find math fascinating, we are today going to talk about [abstract algebra](https://en.wikipedia.org/wiki/Abstract_algebra) and its role in [differential dataflow](https://github.com/TimelyDataflow/differential-dataflow).

The punch-line we are going to build up to is that by intentionally restricting how you interact with a differential dataflow computation, you can make it run a fair bit more efficiently. For example, if you promise that you will only ever add records, and never remove them, we will be able to perform certain graph computations more efficiently.

### A Differential Dataflow Primer

Differential dataflow is a computational model in which we most commonly manipulate collections of data, subject them to interesting and provocative computations, and then *alter* the underlying collections and delight when the computations themselves correctly update.

You can think of a differential dataflow *collection* as an ever-increasing set of triples

    (data, time, diff)

These three fields each describe something interesting about the collection:

1. The `data` entries are some domain-specific information, maybe they each describe an edge in a graph, or a user of some service. Differential dataflow has relatively little opinion on their meaning, requiring mainly that you be able to order them (for reasons that may soon become clear). The computations you describe on these collections probably will look hard at the data, but the meaning you ascribe to the data isn't something we worry about.

2. The `time` entries are the real mind-benders. We could talk forever about these (and encode that amount of time in a `time` entry), but we will not do so here. For this post, let's think of these as unsigned integers that increase as time advances. Super disappointing, I know, but it is about to get better.

3. The `diff` entries are the subject of this post.

Most commonly, we think of the `diff` entries as the signed integers. Their intended meaning is to describe how a collection *changes* as time advances. We form the contents of a collection at some time `t` by accumulating all of the triples `(data, time, diff)` for which `time <= t`. By "accumulating", we mean "for each `data`, add up the associated `diff` entries".

The signed integers make a lot of sense when our collections contain various `data`, each with integer multiplicities. No matter what a collection currently contains, we can change it to any other collection by some signed integers: for each `data` we just take what count we would like the collection to have and subtract whatever the count currently is, that gives us the necessary `diff`.

This is great, and most uses of differential dataflow are really this simple. Signed integers, which allow us to change from any set of counts to any other set of counts.

### Generalizing (previously)

So do we just use `isize`, one of Rust's signed integer types, for our collections?

No, because it turns out that one can be a fair bit more general than this. Perhaps we want to use `i64` or `i32`, or maybe even `i8`, signed integers that use more or fewer bits. Perhaps (wait for it) we want to do something amazing.

Until relatively recently, differential dataflow had a trait `Diff` for types that could be used for the `diff` field of those triples. The trait was essentially a synonym for a collection of other arithmetic traits (addition, subtraction, and negation):

```rust
    /// A type that can be used as a difference in differential dataflow.
    pub trait Diff : Add<Self, Output=Self> + Sub<Self, Output=Self> + Neg<Output=Self> {
        /// Tests if the value is the zero value.
        pub fn is_zero(&self) -> bool { self.eq(Self::zero()) }
        /// Produces a zero value (addition no-op).
        pub fn zero(&self) -> Self;
    }
```

This trait had a few implementations (`isize`, `i64`, `i32`) that allowed us to choose which types we want to use, and describe collections that used varying integer representations.

But! This generality also lets us use another type, created specially for differential dataflow:
```rust
    /// A pair of elements, useable as a difference.
    pub struct DiffPair<R1, R2> {
        /// The first element in the pair.
        pub element1: R1,
        /// The second element in the pair.
        pub element2: R2,
    }
```

This type implements `Add`, `Sub`, `Neg`, and `Diff`, using point-wise operations.

This fancy type lets us pack a lot more information into our `diff` element. Much more than just a change in the count of the number of records. Let's take a peek at an example!

### In-place Aggregation

The [`tpchlike`](https://github.com/TimelyDataflow/differential-dataflow/tree/master/tpchlike) project is an attempt to mock up the 22 [TPC-H](http://www.tpc.org/tpch/) queries, in differential dataflow.

In particular, [the first query](https://github.com/TimelyDataflow/differential-dataflow/blob/master/tpchlike/src/queries/query01.rs) just hits the large relation `lineitem` and asks for a whole bunch of aggregate quantities:

    select
        l_returnflag,
        l_linestatus,
        sum(l_quantity) as sum_qty,
        sum(l_extendedprice) as sum_base_price,
        sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
        sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
        avg(l_quantity) as avg_qty,
        avg(l_extendedprice) as avg_price,
        avg(l_discount) as avg_disc,
        count(*) as count_order
    from
        lineitem
    where
        l_shipdate <= date '1998-12-01' - interval ':1' day (3)
    group by
        l_returnflag,
        l_linestatus

Each of those `l_*` things are attributes in the schema, and each correspond to fields in the associated collection's `data`. Two of them, `l_returnflag` and `l_linestatus`, are grouping keys, seen from:

    group by
        l_returnflag,
        l_linestatus

The `l_shipdate` is used to filter the data (which we do, but don't worry about that now). All of the rest of them, `l_quantity`, `l_extendedprice`, `l_discount`, and `l_tax` are fed in to aggregate quantities, listed here:

        sum(l_quantity) as sum_qty,
        sum(l_extendedprice) as sum_base_price,
        sum(l_extendedprice * (1 - l_discount)) as sum_disc_price,
        sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,
        avg(l_quantity) as avg_qty,
        avg(l_extendedprice) as avg_price,
        avg(l_discount) as avg_disc,
        count(*) as count_order

How should we go about determining these aggregates, and more interestingly maintaining them as the data change? We could group all of our data by `(l_returnflag, l_linestatus)`, producing (as it turns out) four groups containing massive amounts of data.

A better way is to explain to differential dataflow that most of these quantities can be accumulated, in-place. We don't need to see each `l_quantity` entry, we just need to see its total. The same is true of `l_extendedprice`. Other fields also just feed in to aggregates, and as long as we can move the aggregatable data out of `data` and in to `diff`, we will be golden.

Here is the corresponding differential dataflow code:

```rust
    collections
        .lineitems()
        .explode(|item|
            if item.ship_date <= ::types::create_date(1998, 9, 2) {
                Some(((item.return_flag[0], item.line_status[0]),
                    DiffPair::new(item.quantity as isize,
                    DiffPair::new(item.extended_price as isize,
                    DiffPair::new((item.extended_price * (100 - item.discount) / 100) as isize,
                    DiffPair::new((item.extended_price * (100 - item.discount) * (100 + item.tax) / 10000) as isize,
                    DiffPair::new(item.discount as isize, 1)))))))
            }
            else {
                None
            }
        )
        .count_total()
```

Ignore for the moment that `explode` makes very little sense as a name. What we are doing is filtering the `lineitems` relation by date, and then producing as output update triples

    (data, time, diff)

where

1. The `data` are just `(item.return_flag[0], item.line_status[0])`, corresponding to the key fields we want to use to segment the aggregation.
2. The `time` field gets chosen based on the input time of each `item`; we don't get to change that.
3. The `diff` field is, morally at least, the tuple containing the six fields:

        item.quantity,
        item.extended_price,
        item.extended_price * (1.0 - item.discount),
        item.extended_price * (1.0 - item.discount) * (1.0 + item.tax),
        item.discount,
        1,

    The reason for these six may already be clear, and if not then bear with us for just a moment.

As we produce these update triples, many of them will have exactly the same `data` pair of return flag and line status. There are only four distinct pairs, it turns out. Each update triple with the same `data` and `time` will simply have their `diff` fields accumulated. The full set of changes, some 60 million records in the scale-factor 10 dataset, collapse down to only as many distinct `(data, time)` pairs as now exist.

All of that exciting variety in different records is mostly just added up.

The six fields above are chosen so that we can produce the various sums and averages needed by the query. The mystery might be the number `1`, which is there so that we can get a count out, and so that we can find a denominator for all of our averages.

The final call to `count_total()`, up in the Rust code above, produces for each of the four pairs of keys their accumulated six-tuples, whenever they change.

### Generalization (redux)

All of this was possible because we were able to describe more general conditions on what a differential dataflow "collection" can accumulate. I put collection in quotes because it is only partially true that what we are representing is a multi-set of `data`. It is now something a bit more sophisticated but, since we still have a `count`, at least as general.

Informally, we've explained to differential dataflow which fields in `data` are opaque distinct values, and which fields can (and should) just be accumulated up. With this explanation, differential dataflow gets substantially faster and its in-memory representation much more compact.

However, this was old news a long time ago. We (and most sane systems) have been doing this for a while.

### Generalization to Monoids

The conditions we had on `Diff` up above largely correspond to a ["Commutative Group"](https://en.wikipedia.org/wiki/Abelian_group). You should be able to add and subtract elements, and it shouldn't much matter in what order you do this.

Those are pretty great requirements, right? Obviously we want to add things up, and because distribution and whatnot it will be hard to control in what order this happens, probably.

But hey, when do we actually *subtract* things?

It turns out that subtraction shows up in .. approximately one place in differential dataflow. The `group` operator, analogous to MapReduce's `reduce()` operator, allows one to apply arbitrary logic to a collection of values (partitioned by some key). Because that function can change arbitrarily, we might need to *subtract* some elements that were once produced as output but should no longer be produced.

What about other operators, like `join`? Doesn't need subtraction. What about `iterate`? Eh, .. it kinda needs subtraction, but it turns out it doesn't always need it. We will be fixing that in just a moment.

#### Step 1: Generalize `Diff` to `Monoid` and `Abelian`.

Our first step is to dive in and replace the `Diff` trait with `Monoid`, because a group without subtraction or negation ("invertibility") is called a [monoid](https://en.wikipedia.org/wiki/Monoid). It is just

```rust
    /// A type that can be used as a difference in differential dataflow.
    pub trait Monoid : Add<Self, Output=Self> {
        /// Tests if the value is the zero value.
        pub fn is_zero(&self) -> bool { self.eq(Self::zero()) }
        /// Produces a zero value (addition no-op).
        pub fn zero(&self) -> Self;
    }
```

That was super-simple, and now we just check what breaks.

Because Rust is a seriously typed language (I can never recall if "static" or "strongly" is the important one) we get some errors wherever we relied on the `Sub` or `Neg` implementations.

It turns out that it is essentially just in the implementation of the `Group` trait, and in the implementation of `iterate::Variable`, which is how we do iteration (uh oh!).

#### Step 2: Fix `Group`

The `Group` trait is going to need to require subtraction, so we add it as a constraint. You can use the methods from `Group` if your `diff` type implements subtraction, and not otherwise. If otherwise, you still get access to methods like `map`, `filter`, and `join`.

Sweet!

#### Step 3: Fix `iterate::Variable`

This is pretty in the weeds, but it turns out that if you would like to compute the result of an iterative computation in differential dataflow, starting from some `initial` collection,

    f^0(initial) // just "initial"
    f^1(initial)
    f^2(initial)
    ...
    f^i(initial)

then we need to be able to perform subtraction. This has to do with how we set up the initial differences to make sure that while you get a dose of `initial` in the very first iteration, you don't get it added in to each subsequent iteration.

Anyhow, you'll need subtraction for this form of iteration. So we add `Sub` and `Neg` as type constraints on it.

However!!! There is a pretty common form of iteration that does not need subtraction. Imagine that your iterative computation has the form

    f(x) = logic(x) + initial

where you apply some `logic` to the current iterate, and then fold in another copy of the `initial` collection. This form of iteration does *not* require subtraction!

This form of iteration is also pretty common, at least in the workhorse examples of data-parallel computation. When you repeatedly derive new facts, you'll actually want to keep around the old facts anyhow, for correctness and stuff. When you explore shortest paths leading out from some vertex, you want to keep both the new path lengths and all of the previous ones. It turns out that keeping `initial` around does that

**Homework**: explain why just adding `initial` does the job, and you don't need to add `x` instead.

#### Step 4: Generalize `Group`

This is a bit more of a hack, but we are all learning here.

The `group` operator allows you to specify an arbitrary function from an input key, and set of values, to an output set of values. We are able to use subtraction to change any output collection into any other output collection. How might we generalize this when we don't have access to subtraction?

What I came up with is a method `group_solve`, which allows you to specify an arbitrary function from input key, a set of input values, a set of candidate output values, to a set of updates that should be applied to the output. Importantly, you don't get to *specify* the new output. You only get to specify something that will be incorporated in to the current output, which .. is some limited way to influence the new output.

Let's take an example!

Perhaps you are trying to maintain the maximum `l_quantity` associated with any entry of that `lineitem` relation. That seems kind of annoying to do if I am allowed to make arbitrary changes to the inputs; for any input record, I could retract all others making it the correct output, forcing you to remember each input distinctly.

However, a good thing happens if we restrict ourselves to a *monoid*. Let's say that in addition to our exciting six-tuple from above we add a seventh field, `max_quantity` let's call it. Moreover, we make its addition rule be `max(x,y)`. What about subtraction? Well, we don't support subtraction. This propogates backwards in our dataflow to a constraint on the `lineitem` input: we don't get to subtract elements from it; we only get to apply updates with non-negative counts (the `usize` type, as opposed to the `isize` type).

We have intentionally restricted how we interact with our computation, only adding never removing, and as a result we can more efficiently track something. This is only appropriate if the restriction is appropriate: in some contexts we do only add things, but in others we may want the ability to retract inputs.

### A worked example

I went through and put together two examples of the same computation, breadth-first distance labeling, done both in the traditional implementation ([examples/bfs.rs](https://github.com/TimelyDataflow/differential-dataflow/blob/master/examples/bfs.rs)) and in a new implementation ([examples/monoid-bfs.rs](https://github.com/TimelyDataflow/differential-dataflow/blob/master/examples/monoid-bfs.rs)) that relies on `Monoid` and only allows us to add new edges.

Let's see how the two of them compare.

#### Old-school BFS

The traditional BFS implementation is an iterative computation that develops for each node the set of distances at which it can be reached (from some root node). Each pair `(node, dist)` has an integer count, which takes a bit of space (if there are many distances) but is general enough to allow us to make arbitrary input changes.

For reference, the code looks like this:

```rust
// returns pairs (n, s) indicating node n can be reached from a root in s steps.
fn bfs<G: Scope>(edges: &Collection<G, Edge>, roots: &Collection<G, Node>) -> Collection<G, (Node, u32)>
where G::Timestamp: Lattice+Ord {

    // initialize roots as reaching themselves at distance 0
    let nodes = roots.map(|x| (x, 0));

    // repeatedly update minimal distances each node can be reached from each root
    nodes.iterate(|inner| {

        let edges = edges.enter(&inner.scope());
        let nodes = nodes.enter(&inner.scope());

        inner.join_map(&edges, |_k,l,d| (*d, l+1))
             .concat(&nodes)
             .group(|_, s, t| t.push((*s[0].0, 1)))
     })
}
```

#### Monoidal BFS

Instead of using pairs `(node, dist)` with integer counts, we can maintain the collection of elements `node` with .. a freaky weird monoid:

```rust
/// An integer where + is "min" and * is "sum".
pub struct MinSum {
    value: u32,
}

impl std::ops::Add<Self> for MinSum {
    type Output = Self;
    fn add(self, rhs: Self) -> Self {
        MinSum { value: std::cmp::min(self.value, rhs.value) }
    }
}

impl std::ops::Mul<Self> for MinSum {
    type Output = Self;
    fn mul(self, rhs: Self) -> Self {
        MinSum { value: self.value + rhs.value }
    }
}

impl Monoid for MinSum {
    fn zero() -> MinSum { MinSum { value: u32::max_value() } }
}
```

Ok, this is pretty far-out, but it turns out to be what works. Each time we "add" up a bunch of updates we only retain the minimum value. When we `join` with other values, the multiplication we do is addition (adding up path lengths, retaining the minimum).

The code for this version is .. pretty horrible at the moment. You can check it out in the link up above. Morally it is the same, but it lacks a lot of sugar. The most exciting moment, though, is that where we previously retain the least distance using a `group` operation, we now use `group_solve`:

```rust
            .group_solve(|_key, input, output, updates| {
                // compare the diff (.1) of the input and output.
                if output.is_empty() || input[0].1 < output[0].1 {
                    updates.push(((), input[0].1));
                }
            })
```

We only need to produce output updates if the least distance in the input is smaller than that currently reflected in the output (or if the output is empty).

#### Experiments!

Does this actually do anything good at all?

Well, yes kinda but it depends. If you run the computations on random graphs (like I do) with relatively small average degree (like I do) then there isn't that much difference between tracking all of the proffered distances (relatively few) and the least distance (one).

On the other hand, if you crank the average degree up to twenty (not so large, really) and go the additional distance of making edge weights random numbers between zero and 1,000, you get some pretty exciting results.

The old code, which I have to say is a pretty crappy way to compute shortest paths (we do breadth-first exploration by hop count, not by distance), gives us the following for 1,000 edge additions to a 1M node, 20M edge graph:

    Echidnatron% cargo run --release --example sssp -- 1000000 20000000 1000 1 1000 no
        Finished release [optimized + debuginfo] target(s) in 0.16s
         Running `target/release/examples/sssp 1000000 20000000 1000 1 1000 no`
    performing BFS on 1000000 nodes, 20000000 edges:
    1.073813026s    loaded
    210.942974636s  stable
    210.969052581s  0:  25967913
    210.983290373s  1:  13716671
    ...
    225.992076415s  998:    723287
    225.999423139s  999:    7336721
    finished; elapsed: 225.999442906s
    Echidnatron%

Takes a while to get started, and then another 15 seconds to do the 1,000 changes.

The approach where we only get to add edges, on the other hand, goes a fair bit faster. This is also doing breadth-first exploration by hop count, but it hurts a bit less:

    Echidnatron% cargo run --release --example monoid-bfs -- 1000000 20000000 1000 1 1000 no
        Finished release [optimized + debuginfo] target(s) in 0.11s
         Running `target/release/examples/monoid-bfs 1000000 20000000 1000 1 1000 no`
    performing BFS on 1000000 nodes, 20000000 edges:
    953.631951ms    loaded
    38.696207054s   stable
    38.696914646s   0:  673442
    38.697606572s   1:  675772
    ...
    43.006604253s   998:    677619
    43.009548921s   999:    2931505
    finished; elapsed: 43.00957037s
    Echidnatron%

Faster to get started, and then under 5 seconds to do the 1,000 changes.

In both cases we are only adding edges, but in the second case we have specialized the code to understand and exploit this.